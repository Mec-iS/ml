{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network prediction and measuring its effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find the correct weigths descending the prime derivative of the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If $f$ is the activation function, the output of the network is: \n",
    "\n",
    "(__1__): $$y_j^m = f(\\sum_i w_{ij} · x_i^m)$$\n",
    "\n",
    "\n",
    "\n",
    "(the dot product of the tensor containing the weights and the tensor containing the inputs); as activation function it is used the **sigmoid**.\n",
    "* The learning rate is a discretional amount that is set by trial and errors to fit the model.\n",
    "* Every model needs to minimize the error to be effective, to minimize the error it is used the _sum of squared errors_ (SSE).\n",
    "* To find the right amounts for the weights, it is needed to scale down (**descent**) the error proportionally to the output and the learning rate. The best mathematical tool to carry on this task is to descent the **gradient** of the activation function (the prime derivative of the multivariable system that represent the network).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Minimize the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantity to minimize is the difference between the researched output and the output calculated in [1]: \n",
    "\n",
    "(__2__): $$(y_j - y_j^m)$$\n",
    "\n",
    "Using SSE:\n",
    "\n",
    "(__3__): $$E = \\frac{1}{2} \\sum_m \\sum_j [y_j - y_j^m] ^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find the change in weight ($\\Delta w_{ij}$), the amount of the descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right amount to decrease the weights is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the partial derivatives in the second operand of the right side of this equation:\n",
    "\n",
    "(__4__): $$\\Delta w_{ij} = -\\eta \\frac{\\delta E}{\\delta w_{ij}} $$\n",
    "\n",
    "the [chain rule](https://www.khanacademy.org/math/ap-calculus-ab/product-quotient-chain-rules-ab/chain-rule-ab/v/chain-rule-introduction) is needed. The result is:\n",
    "\n",
    "(__5__): $$\\Delta w_{ij} = \\eta * (y_j - y_j^m) * f'(\\sum_i w_{ij} · x_i) * x_i$$\n",
    "\n",
    "where the __change in weight__ is proportional to the error (amplified by the learning rate $\\eta$) and the input $x_i$ and the prime derivative of the dot product of the weights tensor and the input tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(__6__): $$\\Delta w_{ij} = \\eta \\delta_j x_i$$\n",
    "\n",
    "where the __error gradient__ $\\delta_j$ is actually:\n",
    "\n",
    "(__7__): $$\\delta_j = (y_j - y_j^m) * f'(\\sum_i w_{ij} x_i) $$\n",
    "\n",
    "with the first operand to be the error and second operand of the right part of the equation being the __gradient (the prime derivative) of the activation function__ for the dot product of weights and inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37754066879814541"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define sigmoid\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# set initial values for w and x tensors\n",
    "w = np.array([0.5, -0.5])\n",
    "x = np.array([1, 2])\n",
    "learningrate_eta = 0.5\n",
    "\n",
    "# find the output of the network\n",
    "output_y = sigmoid(np.dot(w, x))\n",
    "output_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1775406687981454"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the target error (a discretional value)\n",
    "target_y = np.array(0.5)\n",
    "\n",
    "# find the actual error\n",
    "error = target - output_y\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define sigmoid prime\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# find the error gradient \n",
    "error_gradient = sigmoid_prime(np.dot(w, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the right gradient descent step [0.11750185610079725, 0.23500371220159449]\n"
     ]
    }
   ],
   "source": [
    "# find the appropriate change in weight for every input\n",
    "delta_w = [\n",
    "    learningrate_eta * error_gradient * x[0],\n",
    "    learningrate_eta * error_gradient * x[1]\n",
    "]\n",
    "\n",
    "print('This is the right gradient descent step', delta_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
