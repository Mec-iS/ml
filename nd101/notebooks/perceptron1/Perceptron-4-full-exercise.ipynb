{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([( 0.,  620.,  4.  ,  2.), ( 0.,  560.,  3.04,  3.),\n",
       "       ( 0.,  460.,  2.63,  2.), ( 0.,  700.,  3.65,  2.)], \n",
       "      dtype=[('admit', '<f8'), ('gre', '<f8'), ('gpa', '<f8'), ('rank', '<f8')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data found at http://www.ats.ucla.edu/stat/data/binary.csv\n",
    "points = np.genfromtxt('binary.csv', delimiter=',', names=True)\n",
    "\n",
    "points[-5:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data needs to be integrated with dummy variables: a dummy variable (also known as an indicator variable, design variable, Boolean indicator, categorical variable, binary variable, or qualitative variable) is one that takes the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll also need to standardize the GRE and GPA data, which means to scale the values such they have zero mean and a standard deviation of 1. This is necessary because the sigmoid function squashes really small and really large inputs. The gradient of really small and large inputs is zero, which means that the gradient descent step will go to zero too. Since the GRE and GPA values are fairly large, we have to be really careful about how we initialize the weights or the gradient descent steps will die off and the network won't train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Targets for accuracy test\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2027827  -0.05644616  0.26441774  0.62177434 -0.09559271 -0.09558601]\n",
      "Train loss:  0.262213022107\n",
      "Train loss:  0.21175289463\n",
      "Train loss:  0.204042375284\n",
      "Train loss:  0.201976685332\n",
      "Train loss:  0.201201048563\n",
      "Train loss:  0.200846619991\n",
      "Train loss:  0.200664256824\n",
      "Train loss:  0.200562892968\n",
      "Train loss:  0.2005034469\n",
      "Train loss:  0.200467204385\n",
      "Prediction accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "print(weights)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        #print(x)\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(np.dot(weights, x))\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate change in weights\n",
    "        error_gradient = error * output * (1- output)\n",
    "        #print(\"error gradient\", error_gradient)\n",
    "        del_w += error_gradient * x\n",
    "        #del_w += error * output * (1 - output) * x\n",
    "        #print(\"delta weights\", del_w)\n",
    "\n",
    "        # TODO: Update weights\n",
    "    weights += learnrate * del_w / n_records\n",
    "    #if e > 10: break\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multilayer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Implement a forward pass through a 4x3x2 (4 units/nodes in the input layer, 3 units in the hidden layer, 2 units in the output layer) network, with sigmoid activation functions for both layers. This is a TWO layer network, as inputs are not counted as a layer (first layer on the picture below).\n",
    "\n",
    "Things to do:\n",
    "\n",
    "* Calculate the input to the hidden layer.\n",
    "* Calculate the hidden layer output.\n",
    "* Calculate the input to the output layer.\n",
    "* Calculate the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[ 0.41492192  0.42604313  0.5002434 ]\n",
      "Output-layer Output:\n",
      "[ 0.49815196  0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data (inputs)\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_in_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_out = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = X     # the input layer passes the inputs to the hidden layer\n",
    "hidden_layer_out = sigmoid(np.dot(hidden_layer_in, weights_in_hidden))   # calculate the output for the hidden layer\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = hidden_layer_out   # the inputs for the output layers are the output from the hidden layer\n",
    "output_layer_out = sigmoid(np.dot(output_layer_in, weights_hidden_out))  # calculate the output for the output layer\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "            \n",
    "        +-------------------+\n",
    "        |                   |\n",
    "        | X[1]              |\n",
    "        |                   |                     HIDDEN LAYER\n",
    "        |                   |\n",
    "     i  +-------------------+\n",
    "                        |                      +------------------+\n",
    "                        +---------------------->                  |\n",
    "                                               | sigmoid(         |\n",
    "                        +---------------------->   x[1,2] * w[1,2]|           OUTPUT LAYER\n",
    "     n                  |                      | )                |\n",
    "        +-------------------+                  +------------------+\n",
    "        |                   |                        |                     +-----------------+\n",
    "        | X[2]              |                        +--------------------->                 |\n",
    "        |                   |                                              | sigmoid(X * W)  |           o\n",
    "        |                   |                        +--------------------->                 +------>\n",
    "     p  +-------------------+                        |                     |                 |\n",
    "                        |                      +----------------+          +-----------------+           u\n",
    "                        +---------------------->                |\n",
    "                                               |                |\n",
    "                        +---------------------->                |                                        t\n",
    "                        |                      |                |\n",
    "     u  +-------------------+                  +----------------+\n",
    "        |                   |                        |                     +-----------------+           p\n",
    "        | X[3]              |                        +--------------------->                 |\n",
    "        |                   |                                              |                 |\n",
    "        |                   |                        +--------------------->                 +----->     u\n",
    "        +-------------------+                        |                     |                 |\n",
    "     t                  |                      +---------------+           +-----------------+\n",
    "                        +---------------------->               |                                         t\n",
    "                                               |               |\n",
    "                        +---------------------->               |                                         s\n",
    "                        |                      |               |\n",
    "       +--------------------+                  +---------------+\n",
    "     s |                    |\n",
    "       |  X[4]              |\n",
    "       |                    |\n",
    "       |                    |\n",
    "       +--------------------+\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers. Backpropagation is the \"scaling back\" of the output error to the previous layers to calculate the hidden layer error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The error attributed to each *output unit* k is $\\delta_k^o$, to find the error of each hidden layer output it is needed to be **scaled for the weights and the gradient descent* $h_j$:\n",
    "\n",
    "(__1__):  $$h_j = f'(w_{ij} Â· x_i) $$\n",
    "\n",
    "that is the definition of Gradient Descent for the function $f$ at the point $j$. So the new unit error $\\delta_j^h$ for the hidden layer and the new gradient descent step $\\Delta$ are the scaled values:\n",
    "\n",
    "(__2__):  $$\\delta_j^h = \\sum W_{jk} \\delta_k^o f'(h_j)  $$\n",
    "(__3__):  $$\\Delta_{ij} = \\eta \\delta_j^h x_i  $$ \n",
    "\n",
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_i$ are input unit values. This form holds for however many layers there are. The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48497343085\n",
      "[ 0.00070802 -0.00204471]\n",
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[array([ 0.00017701, -0.00051118]), array([  3.54011093e-05,  -1.02235701e-04]), array([ -7.08022187e-05,   2.04471402e-04])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])  # wi\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])   # W\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)  # h\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)     # a\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)  # W * a\n",
    "output = sigmoid(output_layer_in)     # f(W * a)\n",
    "print(output)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate error\n",
    "# output - target\n",
    "error = target - output\n",
    "\n",
    "h = np.dot(x, weights_input_hidden)\n",
    "a = sigmoid(h)\n",
    "y_cap = sigmoid(np.dot(a, weights_hidden_output))\n",
    "error = target - y_cap\n",
    "\n",
    "# TODO: Calculate error gradient for output layer\n",
    "# error * delta_unit * gradient_descent\n",
    "del_err_output = error * (output * (1 - output))      # (yi - y) * prime_deriv( W * a)\n",
    "\n",
    "# TODO: Calculate error gradient for hidden layer\n",
    "del_err_hidden = np.dot(del_err_output, weights_hidden_output) * \\\n",
    "                 hidden_layer_output * (1 - hidden_layer_output)    # deltaO * W * a * (1 - a)   <<< sigmoid prime of a\n",
    "print(del_err_hidden)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * del_err_output * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_o = [learnrate * del_err_hidden * i for i in x]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
