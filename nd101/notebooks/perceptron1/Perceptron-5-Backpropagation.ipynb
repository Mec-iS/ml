{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers. Backpropagation is the \"scaling back\" of the output error to the previous layers to calculate the hidden layer error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The error attributed to each *output unit* k is $\\delta_k^o$, to find the error of each hidden layer output it is needed to be **scaled for the weights and the gradient descent* $h_j$:\n",
    "\n",
    "(__1__):  $$h_j = f'(w_{ij} · x_i) $$\n",
    "\n",
    "that is the definition of Gradient Descent for the function $f$ at the point $j$. So the new unit error $\\delta_j^h$ for the hidden layer and the new gradient descent step $\\Delta$ are the scaled values:\n",
    "\n",
    "(__2__):  $$\\delta_j^h = \\sum W_{jk} \\delta_k^o f'(h_j)  $$\n",
    "(__3__):  $$\\Delta_{ij} = \\eta \\delta_j^h x_i  $$ \n",
    "\n",
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_i$ are input unit values. This form holds for however many layers there are. The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48497343085\n",
      "[ 0.00070802 -0.00204471]\n",
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[array([ 0.00017701, -0.00051118]), array([  3.54011093e-05,  -1.02235701e-04]), array([ -7.08022187e-05,   2.04471402e-04])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])  # wi\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])   # W\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)  # h\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)     # a\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)  # W * a\n",
    "output = sigmoid(output_layer_in)     # f(W * a)\n",
    "print(output)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate error\n",
    "# output - target\n",
    "error = target - output\n",
    "\n",
    "h = np.dot(x, weights_input_hidden)\n",
    "a = sigmoid(h)\n",
    "y_cap = sigmoid(np.dot(a, weights_hidden_output))\n",
    "error = target - y_cap\n",
    "\n",
    "# TODO: Calculate error gradient for output layer\n",
    "# error * delta_unit * gradient_descent\n",
    "del_err_output = error * (output * (1 - output))      # (yi - y) * prime_deriv( W * a)\n",
    "\n",
    "# TODO: Calculate error gradient for hidden layer\n",
    "del_err_hidden = np.dot(del_err_output, weights_hidden_output) * \\\n",
    "                 hidden_layer_output * (1 - hidden_layer_output)    # deltaO * W * a * (1 - a)   <<< sigmoid prime of a\n",
    "print(del_err_hidden)\n",
    "del_err_hidden_2 = np.dot(del_err_output, weights_hidden_output) * \\\n",
    "                   sigmoid_prime(hidden_layer_output)\n",
    "    \n",
    "assert del_err_hidden.all() == del_err_hidden_2.all()\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * del_err_output * hidden_layer_output  # eta * deltaO * a\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_o = [learnrate * del_err_hidden * i for i in x]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "+------+\n",
    "| x[0] |\n",
    "+----+-+                      HIDDEN LAYER             OUTPUT LAYER\n",
    "     |\n",
    "     |         w[0], w[1]\n",
    "     |                      +----------------+      +-------------------+\n",
    "     +--------------------->|        W       |      |                   |\n",
    "                            | sigmoid(x * w) |      | sigmoid(a * W)    |\n",
    "     +--------------------->|                +----->                    +------>  y\n",
    "     |                      |       a        |      |         h         |\n",
    "     |                      +----------------+      +-------------------+\n",
    "+----+-+\n",
    "| x[1] |\n",
    "+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Error gradient for output layer** `del_err_output`: \n",
    "\n",
    "(__4__): $$\\delta^o = (1 - y) f'(W·a)$$\n",
    "\n",
    "if target error is set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Error gradient for hidden layer** `del_err_hidden`:\n",
    "\n",
    "(__5__): $$\\delta^h = W · \\delta^o * f'(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Change in weights for hidden layer to output layer** `delta_w_h_o`:\n",
    "\n",
    "(__6__): $$\\Delta W = \\eta \\delta^o * a$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Change in weights for input layer to hidden layer** `delta_w_i_o`:\n",
    "\n",
    "(__7__): $$\\Delta w_i = \\eta \\delta^h w_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.229409303659\n",
      "Train loss:  0.230251430055   WARNING - Loss Increasing\n",
      "Train loss:  0.231048286585   WARNING - Loss Increasing\n",
      "Train loss:  0.231805931256   WARNING - Loss Increasing\n",
      "Train loss:  0.232529286151   WARNING - Loss Increasing\n",
      "Train loss:  0.233222274515   WARNING - Loss Increasing\n",
      "Train loss:  0.233887980345   WARNING - Loss Increasing\n",
      "Train loss:  0.234528802409   WARNING - Loss Increasing\n",
      "Train loss:  0.235146591731   WARNING - Loss Increasing\n",
      "Train loss:  0.235742769471   WARNING - Loss Increasing\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 3  # number of hidden units\n",
    "epochs = 500\n",
    "learnrate = 0.05\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden) # h = w * x\n",
    "        hidden_activations = sigmoid(hidden_input)  # a = f(h)\n",
    "        output = sigmoid(np.dot(weights_hidden_output, hidden_activations))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate error gradient in output unit\n",
    "        output_error = error * (weights_hidden_output * hidden_activations*(1-weights_hidden_output * hidden_activations))\n",
    "\n",
    "        # TODO: propagate errors to hidden layer\n",
    "        hidden_error = np.dot(weights_hidden_output, output_error) * (hidden_activations * (1-hidden_activations))\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output +=  output_error * hidden_activations\n",
    "        del_w_input_hidden += hidden_error * x[:, None]   # add None values to adjust dimension\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_activations = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_activations,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
