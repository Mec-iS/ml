{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers. Backpropagation is the \"scaling back\" of the output error to the previous layers to calculate the hidden layer error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error attributed to each *output unit* k is $\\delta_k^o$, to find the error of each hidden layer output it is needed to be **scaled for the weights and the gradient descent* $h_j$:\n",
    "\n",
    "(__1__):  $$h_j = f'(w_{ij} · x_i) $$\n",
    "\n",
    "that is the definition of Gradient Descent for the function $f$ at the point $j$. So the new unit error $\\delta_j^h$ for the hidden layer and the new gradient descent step $\\Delta$ are the scaled values:\n",
    "\n",
    "(__2__):  $$\\delta_j^h = \\sum W_{jk} \\delta_k^o f'(h_j)  $$\n",
    "(__3__):  $$\\Delta_{ij} = \\eta \\delta_j^h x_i  $$ \n",
    "\n",
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_i$ are input unit values. This form holds for however many layers there are. The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48497343085\n",
      "[ 0.00070802 -0.00204471]\n",
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[array([ 0.00017701, -0.00051118]), array([  3.54011093e-05,  -1.02235701e-04]), array([ -7.08022187e-05,   2.04471402e-04])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])  # wi\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])   # W\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)  # h\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)     # a\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)  # W * a\n",
    "output = sigmoid(output_layer_in)     # f(W * a)\n",
    "print(output)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate error\n",
    "# output - target\n",
    "error = target - output\n",
    "\n",
    "h = np.dot(x, weights_input_hidden)\n",
    "a = sigmoid(h)\n",
    "y_cap = sigmoid(np.dot(a, weights_hidden_output))\n",
    "error = target - y_cap\n",
    "\n",
    "# TODO: Calculate error gradient for output layer\n",
    "# error * delta_unit * gradient_descent\n",
    "del_err_output = error * (output * (1 - output))      # (yi - y) * prime_deriv( W * a)\n",
    "\n",
    "# TODO: Calculate error gradient for hidden layer\n",
    "del_err_hidden = np.dot(del_err_output, weights_hidden_output) * \\\n",
    "                 hidden_layer_output * (1 - hidden_layer_output)    # deltaO * W * a * (1 - a)   <<< sigmoid prime of a\n",
    "print(del_err_hidden)\n",
    "del_err_hidden_2 = np.dot(del_err_output, weights_hidden_output) * \\\n",
    "                   sigmoid_prime(hidden_layer_output)\n",
    "    \n",
    "assert del_err_hidden.all() == del_err_hidden_2.all()\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * del_err_output * hidden_layer_output  # eta * deltaO * a\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_o = [learnrate * del_err_hidden * i for i in x]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+------+\n",
    "| x[0] |\n",
    "+----+-+                               HIDDEN LAYER                              OUTPUT LAYER\n",
    "     |\n",
    "     |         w[0], w[1]\n",
    "     |                                 +----------------+                   +-------------------+\n",
    "     +--------------------------------->                |        W          |                   |\n",
    "                                       | sigmoid(x * w) |                   | sigmoid(a * W)    |\n",
    "     +--------------------------------->                +------------------->                   +------>  y\n",
    "     |                                 |       a        |                   |         h         |\n",
    "     |                                 +----------------+                   +-------------------+\n",
    "+----+-+\n",
    "| x[1] |\n",
    "+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error gradient for output layer** `del_err_output`: \n",
    "\n",
    "(__4__): $$\\delta^o = (1 - y) f'(W·a)$$\n",
    "\n",
    "if target error is set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error gradient for hidden layer** `del_err_hidden`:\n",
    "\n",
    "(__5__): $$\\delta^h = W * \\delta^o * f'(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change in weights for hidden layer to output layer** `delta_w_h_o`:\n",
    "\n",
    "(__6__): $$\\Delta W = \\eta \\delta^o * a$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change in weights for input layer to hidden layer** `delta_w_i_o`:\n",
    "\n",
    "(__7__): $$\\Delta w_i = \\eta \\delta^h w_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
